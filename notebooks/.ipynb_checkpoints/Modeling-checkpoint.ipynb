{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will **build models to predict the primary cause of the car crash**. In previous notebook **EDA** I've performed the exploratory analysis and built some visualizations. At the end the new dataset was stored as **\"cleaned_data_2.csv\"** file in data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of the Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of the project is to be able to predict the primary causes of the car crashes, I will build different kind of **multiclass classification models and evaluate their performance.** At the end of the notebook I will choose a model that has the **most appropriate parameters and performed the best.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_curve, auc\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import additional files with statistical functions\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import explore_data as ed \n",
    "import model_functions as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "# plt.style.use('seaborn-dark')\n",
    "# sns.set_theme('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ModelHistory** class below will help to **store the information of each model that was built**. I will contain the **name of the model, accuracy scores and additional notes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHistory:\n",
    "    \n",
    "    def __init__(self, random_state=2021):\n",
    "        self.scorer = 'accuracy'\n",
    "        self.history = pd.DataFrame(columns=['Name', 'Accuracy_Score', 'Notes'])\n",
    "\n",
    "    def report(self, pipeline, X, y, name, notes='', cv=10,):\n",
    "        kf = KFold(n_splits=cv, random_state=2021, shuffle=True)\n",
    "        scores = cross_val_score(pipeline, X, y, \n",
    "                                 scoring=self.scorer, cv=kf)\n",
    "        self.log_report(name, scores.mean(), notes)\n",
    "        print('Average Score:', scores.mean())\n",
    "        return scores\n",
    "    \n",
    "    def log_report(self, name, av_score, notes):\n",
    "        frame = pd.DataFrame([[name, av_score, notes]], columns=['Name', 'Accuracy_Score', 'Notes'])\n",
    "        self.history = self.history.append(frame)\n",
    "        self.history = self.history.reset_index(drop=True)\n",
    "        self.history = self.history.sort_values('Accuracy_Score')\n",
    "\n",
    "    def print_error(self, name, error):\n",
    "        print('{} has an average error of ${:.2f}'.format(name, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRASH_RECORD_ID', 'CRASH_YEAR', 'CRASH_MONTH', 'CRASH_HOUR',\n",
       "       'CRASH_DAY_OF_WEEK', 'POSTED_SPEED_LIMIT', 'TRAFFIC_CONTROL_DEVICE',\n",
       "       'DEVICE_CONDITION', 'WEATHER_CONDITION', 'LIGHTING_CONDITION',\n",
       "       'FIRST_CRASH_TYPE', 'TRAFFICWAY_TYPE', 'STRAIGHT_ALIGNMENT',\n",
       "       'GOOD_ROADWAY_SUFACE', 'ROAD_DEFECT', 'DESK_REPORT_TYPE', 'CRASH_TYPE',\n",
       "       'DAMAGE', 'PRIM_CONTRIBUTORY_CAUSE', 'MOST_SEVERE_INJURY',\n",
       "       'INJURIES_TOTAL', 'INJURIES_FATAL', 'INJURIES_INCAPACITATING',\n",
       "       'INJURIES_NON_INCAPACITATING', 'INJURIES_REPORTED_NOT_EVIDENT',\n",
       "       'INJURIES_NO_INDICATION', 'INJURIES_UNKNOWN', 'LATITUDE', 'LONGITUDE',\n",
       "       'PERSON_TYPE', 'MALE_PERSON', 'AGE', 'DRIVERS_LICENSE_STATE',\n",
       "       'SAFETY_EQUIPMENT', 'DRIVER_ACTION', 'DRIVER_VISION',\n",
       "       'PHYSICAL_CONDITION', 'BAC_RESULT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/clean_data_2.csv\", dtype={'CRASH_RECORD_ID': str, 'RD_NO': str})\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of Dataset: 99909\n",
      "                               missing_values_% Data_type\n",
      "CRASH_RECORD_ID                             0.0    object\n",
      "CRASH_YEAR                                  0.0     int64\n",
      "CRASH_MONTH                                 0.0     int64\n",
      "CRASH_HOUR                                  0.0     int64\n",
      "CRASH_DAY_OF_WEEK                           0.0     int64\n",
      "POSTED_SPEED_LIMIT                          0.0     int64\n",
      "TRAFFIC_CONTROL_DEVICE                      0.0    object\n",
      "DEVICE_CONDITION                            0.0    object\n",
      "WEATHER_CONDITION                           0.0    object\n",
      "LIGHTING_CONDITION                          0.0    object\n",
      "FIRST_CRASH_TYPE                            0.0    object\n",
      "TRAFFICWAY_TYPE                             0.0    object\n",
      "STRAIGHT_ALIGNMENT                          0.0     int64\n",
      "GOOD_ROADWAY_SUFACE                         0.0     int64\n",
      "ROAD_DEFECT                                 0.0     int64\n",
      "DESK_REPORT_TYPE                            0.0     int64\n",
      "CRASH_TYPE                                  0.0    object\n",
      "DAMAGE                                      0.0    object\n",
      "PRIM_CONTRIBUTORY_CAUSE                     0.0    object\n",
      "MOST_SEVERE_INJURY                          0.0    object\n",
      "INJURIES_TOTAL                              0.0     int64\n",
      "INJURIES_FATAL                              0.0     int64\n",
      "INJURIES_INCAPACITATING                     0.0     int64\n",
      "INJURIES_NON_INCAPACITATING                 0.0     int64\n",
      "INJURIES_REPORTED_NOT_EVIDENT               0.0     int64\n",
      "INJURIES_NO_INDICATION                      0.0     int64\n",
      "INJURIES_UNKNOWN                            0.0     int64\n",
      "LATITUDE                                    0.0   float64\n",
      "LONGITUDE                                   0.0   float64\n",
      "PERSON_TYPE                                 0.0    object\n",
      "MALE_PERSON                                 0.0     int64\n",
      "AGE                                         0.0     int64\n",
      "DRIVERS_LICENSE_STATE                       0.0    object\n",
      "SAFETY_EQUIPMENT                            0.0    object\n",
      "DRIVER_ACTION                               0.0    object\n",
      "DRIVER_VISION                               0.0    object\n",
      "PHYSICAL_CONDITION                          0.0    object\n",
      "BAC_RESULT                                  0.0    object\n"
     ]
    }
   ],
   "source": [
    "ed.show_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent models from **overfitting** and to be able to **accurately evaluate** model I will split the data to **X as a features and y as a target variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['CRASH_RECORD_ID', 'PERSON_TYPE', 'DRIVERS_LICENSE_STATE', \n",
    "                   'SAFETY_EQUIPMENT'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"PRIM_CONTRIBUTORY_CAUSE\", axis=1)\n",
    "y = df['PRIM_CONTRIBUTORY_CAUSE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, split the data to **test - train sets** and then **split the train set into test and train sets too.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, X_hold, y_all, y_hold = train_test_split(X, y, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, random_state=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check **how balanced is** my **target classes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AGRESSIVE/IMPROPER DRIVING    0.549521\n",
       "IRRESPONSIBLE BEHAVIOR        0.333250\n",
       "EXTERNAL/OTHER FACTORS        0.117228\n",
       "Name: PRIM_CONTRIBUTORY_CAUSE, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the ratios, I can say that the **data is mostly imbalanced.** Thus, I will use **SMOTE to resample training sets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model with Numeric Features Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **Baseline Model** I will build **Logistic Regression Model** with only **numerical features**. Before building, I will **scale data and resample.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train.select_dtypes(exclude='object').copy()\n",
    "X_test_num = X_test.select_dtypes(exclude='object').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pipeline to Scale the Train Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the **testing data will not leak into training data** while doing cross_validation, I will create a **pipeline** to pass the **SMOTE, StandardScaler and LogisticRegression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_pipeline = make_pipeline(StandardScaler(), SMOTE(random_state = 2021), LogisticRegression(multi_class='multinomial',\n",
    "                             C=0.01, random_state=2021))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will **pass the pipeline into ModelHistory function** to perform **cross-validation and store the model results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = ModelHistory()\n",
    "history.report(scaler_pipeline,X_train_num, y_train, 'Logistic Regression - Defaults', \n",
    "               'Regression with Only Numeric Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Results:** Since the score that cross validation is based on is **accuracy score, the average accuracy score for the trainig set of Model 1 is 0.4486.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaler_pipeline.fit(X_train_num, y_train)\n",
    "scaler_pipeline.score(X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** The **Recall Score for Test Set is 0.4493.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict on trainig and test sets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = scaler_pipeline.predict(X_train_num)\n",
    "test_preds = scaler_pipeline.predict(X_test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the Metrics of the Model 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.print_metrics(y_train,train_preds) #Train Set\n",
    "mf.print_metrics(y_test,test_preds) #Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix for Model 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "plot_confusion_matrix(scaler_pipeline, X_test_num, y_test,\n",
    "                     cmap=plt.cm.Blues,normalize='true')\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 Results:\n",
    "Based on the **Scores of the Training and Test Sets**, there is **no overfitting** - The Accuracy Score for **Training Model is 0.4485,** while the same score for **Testing Model is 0.4489.** But the coefficients of confusion matrix are not ideal. The model predicts **50% of the agressive driving as a irrespomsible behavior.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model with All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first model did not do so well, I will try to **use all features on the second model** and perform **Logistic Regression** also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColumnTransformer for Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I start any modeling, I will use **ColumnTransformer** and **OneHotEncode** all categorical features of the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.select_dtypes(include='object').columns\n",
    "indices = []\n",
    "for col in cols:\n",
    "    indices.append(X_train.columns.get_loc(col))\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(transformers=[('categorical', OneHotEncoder(handle_unknown = 'ignore'), indices)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a new **pipeline** with transformer and perform the **cross-validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_pipeline = make_pipeline(transformer, StandardScaler(with_mean = False), SMOTE(), \n",
    "                               LogisticRegression(multi_class='multinomial',\n",
    "                                                  C=0.01, random_state=2021, max_iter = 5000))\n",
    "\n",
    "categ_pipeline.fit(X_train, y_train)\n",
    "categ_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history.report(categ_pipeline,X_train, y_train, 'Logistic Regression - multinomial', \n",
    "               'Regression with Numeric and Categorical Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Results:** The average **accuracy score for the Model 2 is 0.7254.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_pipeline.fit(X_train, y_train)\n",
    "categ_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict on test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = categ_pipeline.predict(X_train)\n",
    "test_preds = categ_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the Metrics of the Model 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.print_metrics(y_train,train_preds)\n",
    "mf.print_metrics(y_test,test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "plot_confusion_matrix(categ_pipeline, X_test, y_test,\n",
    "                     cmap=plt.cm.Blues,normalize='true')\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance in Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta_values = {name:coef for name, coef in zip(list(X_test.columns), list(log_reg.coef_[0]))}\n",
    "# col = [item[0] for item in sorted(beta_values.items(), key= lambda kv: kv[1], reverse=True)]\n",
    "# beta = [item[1] for item in sorted(beta_values.items(), key= lambda kv: kv[1], reverse=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "# plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# bars = ax.barh(col,beta, color='g')\n",
    "  \n",
    "# for bar in range(1,8):\n",
    "#     bars[-bar].set_color('red')\n",
    "\n",
    "# ax.set_title('Relative Importance of Features');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance = categ_pipeline['logisticregression'].coef_[0]\n",
    "# # summarize feature importance\n",
    "# for i,v in enumerate(importance):\n",
    "# \tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# # plot feature importance\n",
    "# plt.bar([x for x in range(len(importance))], importance)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 Results:\n",
    "\n",
    "Based on the **Scores of the Training and Test Sets**, there is **no overfitting** - The Accuracy Score for **Training Model is 0.7266,** while the same score for **Testing Model is 0.7284.** In terms of matrix coefficients, this model performed much better, considering **the diagonal of true values is high.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch for Best C  - Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "parameters = dict(logisticregression__C=C)\n",
    "\n",
    "clf_GS = GridSearchCV(categ_pipeline, parameters)\n",
    "\n",
    "clf_GS.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Best C:', clf_GS.best_estimator_.get_params()['logisticregression__C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_pipeline_2 = make_pipeline(transformer, StandardScaler(with_mean = False), SMOTE(),  \n",
    "                                 LogisticRegression(multi_class='multinomial',\n",
    "                                                    C=clf_GS.best_estimator_.get_params()['logisticregression__C'], \n",
    "                                                    random_state=2021, max_iter = 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.report(categ_pipeline_2,X_train, y_train, 'Logistic Regression - Multinomial', \n",
    "               'Regression with Numeric and Categorical Features and Best C - Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_pipeline_2.fit(X_train, y_train)\n",
    "categ_pipeline_2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = categ_pipeline_2.predict(X_train)\n",
    "test_preds = categ_pipeline_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.print_metrics(y_train,train_preds)\n",
    "mf.print_metrics(y_test,test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "plot_confusion_matrix(categ_pipeline_2, X_test, y_test,\n",
    "                     cmap=plt.cm.Blues,normalize='true')\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 Results:\n",
    "It is clear that Model 2 **Logistic Regression with all Features did much better:**\n",
    "* The is **no overfitting.**\n",
    "* The confusion matrix shows **good coefficients.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: K-Nearest Neighbors with Only Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will build a pipeline with **KNN Model with only numerical features** to compare it's performance with Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_pipeline = make_pipeline(StandardScaler(), SMOTE(), KNeighborsClassifier())\n",
    "history.report(knn_pipeline,X_train_num, y_train, 'KNN - Defaults', \n",
    "               'KNN with Only Numeric Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit and Score the Training and Testing Sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline.fit(X_train_num, y_train)\n",
    "knn_pipeline.score(X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "knn_pipeline.score(X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** The **Recall Score for Test Set is 0.4527.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict on trainig and test sets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = knn_pipeline.predict(X_train_num)\n",
    "test_preds = knn_pipeline.predict(X_test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the Metrics of the Model 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.print_metrics(y_train,train_preds)\n",
    "mf.print_metrics(y_test,test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix for Model 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "plot_confusion_matrix(knn_pipeline, X_test_num, y_test,\n",
    "                     cmap=plt.cm.Blues,normalize='true')\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 Results:\n",
    "* The model is **overfitting trainig data.**\n",
    "* Confusion Matrix shows **worse results than for Logistic Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: K-Nearest Neighbors with Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the KNN will work for **all features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_knn_pipeline = make_pipeline(transformer, StandardScaler(with_mean = False), SMOTE(), KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.report(cat_knn_pipeline,X_train, y_train, 'KNN - Defaults', \n",
    "               'KNN with All Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_knn_pipeline.fit(X_train, y_train)\n",
    "cat_knn_pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_knn_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** The **Recall Score for Test Set is 0.4493.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict on trainig and test sets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = cat_knn_pipeline.predict(X_train)\n",
    "test_preds = cat_knn_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the Metrics of the Model 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.print_metrics(y_train,train_preds)\n",
    "mf.print_metrics(y_test,test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix for Model 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "plot_confusion_matrix(cat_knn_pipeline, X_test, y_test,\n",
    "                     cmap=plt.cm.Blues,normalize='true')\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 Results:\n",
    "* The model is  **overfitting trainig data.**\n",
    "* Confusion Matrix shows **worse results than for Logistic Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch for Best K - value of KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'kneighborsclassifier__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "\n",
    "clf_GS = GridSearchCV(cat_knn_pipeline, parameters, cv=5, verbose= 0)\n",
    "\n",
    "clf_GS.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Best K - Value:', clf_GS.best_estimator_.get_params()['kneighborsclassifier__n_neighbors'])\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since GridSearch showed that best K - Value for KNN is 4, I will try to build a model with specified parameter and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_knn_pipeline_2 = make_pipeline(transformer, StandardScaler(with_mean = False), SMOTE(), \n",
    "                                 KNeighborsClassifier(n_neighbors = 4))\n",
    "history.report(cat_knn_pipeline_2,X_train, y_train, 'KNN', \n",
    "               'KNN with All Features and Best K - Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_knn_pipeline_2.fit(X_train, y_train)\n",
    "cat_knn_pipeline_2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_knn_pipeline_2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** The **Recall Score for Test Set is 0.4493.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict on trainig and test sets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = cat_knn_pipeline_2.predict(X_train)\n",
    "test_preds = cat_knn_pipeline_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the Metrics of the Model 4 with K - Value 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.print_metrics(y_train,train_preds)\n",
    "mf.print_metrics(y_test,test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix for Model 4 with K - Value 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "plot_confusion_matrix(cat_knn_pipeline_2, X_test, y_test,\n",
    "                     cmap=plt.cm.Blues,normalize='true')\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:** \n",
    "- Model is overfitting the trainig set.\n",
    "- The coeffients are higher for Agressive driving, but lower for the other two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5: Decision Tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will build a **Decision Tree Classifier** and evaluate it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipeline = make_pipeline (transformer, SMOTE(random_state = 2021), StandardScaler(with_mean = False),\n",
    "                               DecisionTreeClassifier(random_state = 2021, max_depth = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.report(tree_pipeline,X_train, y_train, 'DescisionTree', \n",
    "               'Tree with Max-Depth = 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipeline.fit(X_train, y_train)\n",
    "tree_pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = tree_pipeline.predict(X_train)\n",
    "test_preds = tree_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.print_metrics(y_train,train_preds)\n",
    "mf.print_metrics(y_test,test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "plot_confusion_matrix(tree_pipeline, X_test, y_test,\n",
    "                     cmap=plt.cm.Blues,normalize='true')\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 Results:\n",
    "* The model is **not overfitting.**\n",
    "* Confusion Matrix shows **better results than KNN**, but still **worse results than for Logistic Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipeline = make_pipeline(transformer,SMOTE(random_state = 2021), StandardScaler(with_mean = False), \n",
    "                            RandomForestClassifier(max_depth = 10, \n",
    "                                                   min_samples_leaf = 3,\n",
    "                                                   min_samples_split = 4,\n",
    "                                                   n_estimators = 200))\n",
    "history.report(rfc_pipeline,X_train, y_train, 'Random Forest', \n",
    "               'Random Forest with Max-Depth = 10, n estimators = 200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipeline.fit(X_train, y_train)\n",
    "rfc_pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = rfc_pipeline.predict(X_train)\n",
    "test_preds = rfc_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.print_metrics(y_train,train_preds)\n",
    "mf.print_metrics(y_test,test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "plot_confusion_matrix(rfc_pipeline, X_test, y_test,\n",
    "                     cmap=plt.cm.Blues,normalize='true')\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6 Results:\n",
    "* The model is **not overfitting.**\n",
    "* Confusion Matrix shows **same results as Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the metrics of all models that were built, it is clear that Logistic Regression has performed the best. The Accuracy Score of the model is 0.725401. There is no overfitting presented in model and the confusion matrix of true values are highest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will perform final evaluation of the Logistic Regression Model with the data that wasn't used while building. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_pipeline.fit(X_all, y_all)\n",
    "\n",
    "categ_pipeline.score(X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_pipeline.score(X_hold, y_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = categ_pipeline.predict(X_all)\n",
    "test_preds = categ_pipeline.predict(X_hold)\n",
    "mf.print_metrics(y_all,train_preds)\n",
    "mf.print_metrics(y_hold,test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "plot_confusion_matrix(categ_pipeline, X_hold, y_hold,\n",
    "                     cmap=plt.cm.Blues,normalize='true')\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
